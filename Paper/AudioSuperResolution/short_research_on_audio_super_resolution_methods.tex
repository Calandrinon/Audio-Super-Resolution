\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage[backend=biber]{biblatex}
\addbibresource{references.bib}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{hyperref}
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{A short research on audio super-resolution methods \\
}

\author{
\IEEEauthorblockN{Şut George-Mihai}
\IEEEauthorblockA{\textit{3rd-year undergraduate, Computer Science} \\
\textit{Babeş-Bolyai University}\\
georgesut@yahoo.com}
}

\maketitle

\begin{abstract}
Audio super-resolution refers to the task of improving the sound quality of a recording, usually by feeding a downsampled and interpolated signal to a model which produces a "super-resolution" reconstruction of the original signal.
\end{abstract}

\section{Introduction}
	In this short paper, the goal is to investigate the methods discovered so far for audio super-resolution, a topic mainly inspired by image super-resolution (\textcite{ledig2017photorealistic}) and especially by time-series super-resolution. 
\\

\section{Audio super-resolution using neural nets, 2017}

	One of the leading deep learning-oriented works on this topic is \textcite{kuleshov2017audio}, which introduces a convolutional neural net architecture similar to U-Net's bottleneck structure (\textcite{ronneberger2015unet}), whose goal is to upsample an audio signal as a solution to the well-known signal processing problem of bandwidth extension (i.e expanding the frequency range of a signal). The model is trained on data containing high-resolution audio clips mapped to their low-resolution counterparts obtained by downsampling clips from VCTK, a popular speech dataset, and the PIANO dataset. 

	There are essentially two reference points to which the problem solved by \textcite{kuleshov2017audio} is compared: cubic spline interpolation and the dense neural network described in \textcite{lietal2015}, which targets the prediction of the phase and magnitude of the high frequencies in the signal. The loss function used is the mean-squared error, computing the sum of the squared differences between the low- and high-resolution signals, while the main metric that is highlighted is the signal-to-noise $ SNR $ ratio, often used in the signal processing domain. 

	The evaluation results show that the model outperforms the other referenced tasks, a fact which is also underlined by the MUSHRA test of individuals' ratings.

	Concluding the study, some of the impediments of the model are the lack of diverse data and the requirement for solid computing power, leading to results for a music dataset that are weaker than the cubic spline interpolation baseline.

\section{Time-Frequency Networks for Audio Super-Resolution, 2018}

	A significant improvement for the previous paper (\textcite{kuleshov2017audio}) is introduced in \textcite{timefrequencynetworks2018}, in which both the time-domain and the frequency-domain representation of the audio signal are used for the super-resolution task. 

	The proposed model is composed out of an encoder-decoder which contains two branches for processing the frequency-domain representation of the signal and the time-domain representation. 
	Low-resolution inputs to the network are upsampled by using bicubic interpolation, then on the frequency-domain branch, the spectral replicator receives the spectrograms whose lower frequencies are replicated by a specific factor, followed by the AudioUNet (\textcite{kuleshov2017audio}) and the concatenation layer. The branch associated with the time domain also uses an AudioUNet. Before creating the output, a spectral fusion layer merges the reconstructed high-resolution audio signal and its spectral magnitude into the final result.

	To train the network, the L2 loss is used together with regularization. Just as in \textcite{kuleshov2017audio}, the VCTK dataset is resampled and split into the 88/6/6 proportions for the training, testing and validation datasets. The evaluation metrics are also identical to the ones found in the previously mentioned paper.
	
	Finally, the obtained results surpass the ones found in \textcite{kuleshov2017audio} proving that the inclusion of both of the signal representations inside the network is an approach which, as underlined in the ablation tests, leads to slight refinements of the model.   
	
\printbibliography
\vspace{12pt}

\end{document}


