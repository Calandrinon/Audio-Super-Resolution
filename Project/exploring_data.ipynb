{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Exploratory data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We extract one sample from the VCTK dataset and obtain the recording as a time series in a numpy array with the transcript of the text read by a person."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (/home/calandrinon/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/tensorflow/python/keras/layers/normalization/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "Input \u001B[0;32mIn [5]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mconstants\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;241m*\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtfds\u001B[39;00m\n\u001B[1;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m      4\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mDatasetGenerator\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m DatasetGenerator\n",
      "File \u001B[0;32m~/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/tensorflow_datasets/__init__.py:43\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;124;03m\"\"\"`tensorflow_datasets` (`tfds`) defines a collection of datasets ready-to-use with TensorFlow.\u001B[39;00m\n\u001B[1;32m     18\u001B[0m \n\u001B[1;32m     19\u001B[0m \u001B[38;5;124;03mEach dataset is defined as a `tfds.core.DatasetBuilder`, which encapsulates\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     34\u001B[0m \u001B[38;5;124;03m* [Add a dataset](https://www.tensorflow.org/datasets/add_dataset)\u001B[39;00m\n\u001B[1;32m     35\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m     36\u001B[0m \u001B[38;5;66;03m# pylint: enable=line-too-long\u001B[39;00m\n\u001B[1;32m     37\u001B[0m \u001B[38;5;66;03m# pylint: disable=g-import-not-at-top,g-bad-import-order,wrong-import-position,unused-import\u001B[39;00m\n\u001B[1;32m     38\u001B[0m \n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# needs to happen before anything else, since the imports below will try to\u001B[39;00m\n\u001B[1;32m     42\u001B[0m \u001B[38;5;66;03m# import tensorflow, too.\u001B[39;00m\n\u001B[0;32m---> 43\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf_compat\n\u001B[1;32m     44\u001B[0m tf_compat\u001B[38;5;241m.\u001B[39mensure_tf_install()\n\u001B[1;32m     46\u001B[0m \u001B[38;5;66;03m# Imports for registration\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/tensorflow_datasets/core/__init__.py:30\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     23\u001B[0m tf_compat\u001B[38;5;241m.\u001B[39mensure_tf_install()\n\u001B[1;32m     25\u001B[0m \u001B[38;5;66;03m# pylint:disable=g-import-not-at-top\u001B[39;00m\n\u001B[1;32m     26\u001B[0m \n\u001B[1;32m     27\u001B[0m \u001B[38;5;66;03m# pylint: disable=g-bad-import-order\u001B[39;00m\n\u001B[1;32m     28\u001B[0m \u001B[38;5;66;03m# Allow to use `tfds.core.Path` in dataset implementation which seems more\u001B[39;00m\n\u001B[1;32m     29\u001B[0m \u001B[38;5;66;03m# natural than having to import a third party module.\u001B[39;00m\n\u001B[0;32m---> 30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mepath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[1;32m     31\u001B[0m \u001B[38;5;66;03m# pylint: enable=g-bad-import-order\u001B[39;00m\n\u001B[1;32m     33\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow_datasets\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcore\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m community  \u001B[38;5;66;03m# pylint: disable=g-bad-import-order\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/etils/epath/__init__.py:18\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[38;5;124;03m\"\"\"Public API.\"\"\"\u001B[39;00m\n\u001B[1;32m     17\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mepath\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mabstract_path\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Path\n\u001B[0;32m---> 18\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mepath\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mregister\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m register_path_cls\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mepath\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresource_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m resource_path\n\u001B[1;32m     20\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mepath\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mresource_utils\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m to_write_path\n",
      "File \u001B[0;32m~/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/etils/epath/register.py:22\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     19\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Callable, Dict, Tuple, Type, TypeVar\n\u001B[1;32m     21\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mepath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m abstract_path\n\u001B[0;32m---> 22\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mepath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m gpath\n\u001B[1;32m     23\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mepath\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PathLike  \u001B[38;5;66;03m# pylint: disable=g-multiple-import\u001B[39;00m\n\u001B[1;32m     25\u001B[0m _T \u001B[38;5;241m=\u001B[39m TypeVar(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_T\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/etils/epath/gpath.py:28\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mepath\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m abstract_path\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01metils\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mepath\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtyping\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m PathLike\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01mtf\u001B[39;00m\n\u001B[1;32m     30\u001B[0m _P \u001B[38;5;241m=\u001B[39m TypeVar(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m_P\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[1;32m     32\u001B[0m URI_PREFIXES \u001B[38;5;241m=\u001B[39m (\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgs://\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124ms3://\u001B[39m\u001B[38;5;124m'\u001B[39m)\n",
      "File \u001B[0;32m~/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/tensorflow/__init__.py:41\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     38\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msix\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_six\u001B[39;00m\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m \u001B[38;5;21;01m_sys\u001B[39;00m\n\u001B[0;32m---> 41\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mtools\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m module_util \u001B[38;5;28;01mas\u001B[39;00m _module_util\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mutil\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlazy_loader\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LazyLoader \u001B[38;5;28;01mas\u001B[39;00m _LazyLoader\n\u001B[1;32m     44\u001B[0m \u001B[38;5;66;03m# Make sure code inside the TensorFlow codebase can use tf2.enabled() at import.\u001B[39;00m\n",
      "File \u001B[0;32m~/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/tensorflow/python/__init__.py:48\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     46\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m data\n\u001B[1;32m     47\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m distribute\n\u001B[0;32m---> 48\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m keras\n\u001B[1;32m     49\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mfeature_column\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m feature_column_lib \u001B[38;5;28;01mas\u001B[39;00m feature_column\n\u001B[1;32m     50\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layers\n",
      "File \u001B[0;32m~/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/tensorflow/python/keras/__init__.py:27\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tf2\n\u001B[1;32m     26\u001B[0m \u001B[38;5;66;03m# See b/110718070#comment18 for more details about this import.\u001B[39;00m\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m models\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01minput_layer\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Input\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01msequential\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m Sequential\n",
      "File \u001B[0;32m~/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/tensorflow/python/keras/models.py:27\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m optimizer_v1\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional\n\u001B[0;32m---> 27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m sequential\n\u001B[1;32m     28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m training\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m training_v1\n",
      "File \u001B[0;32m~/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/tensorflow/python/keras/engine/sequential.py:28\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     26\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m ops\n\u001B[1;32m     27\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mframework\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m tensor_util\n\u001B[0;32m---> 28\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m layers \u001B[38;5;28;01mas\u001B[39;00m layer_module\n\u001B[1;32m     29\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m base_layer\n\u001B[1;32m     30\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mengine\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m functional\n",
      "File \u001B[0;32m~/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/tensorflow/python/keras/layers/__init__.py:177\u001B[0m, in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    174\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnoise\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m GaussianDropout\n\u001B[1;32m    176\u001B[0m \u001B[38;5;66;03m# Normalization layers.\u001B[39;00m\n\u001B[0;32m--> 177\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnormalization\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m LayerNormalization\n\u001B[1;32m    178\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mtensorflow\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mpython\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mkeras\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mlayers\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mnormalization_v2\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m SyncBatchNormalization\n\u001B[1;32m    180\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tf2\u001B[38;5;241m.\u001B[39menabled():\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'LayerNormalization' from 'tensorflow.python.keras.layers.normalization' (/home/calandrinon/anaconda3/envs/AudioSuperResolution/lib/python3.8/site-packages/tensorflow/python/keras/layers/normalization/__init__.py)"
     ]
    }
   ],
   "source": [
    "from constants import *\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from DatasetGenerator import DatasetGenerator\n",
    "import numpy as np\n",
    "from metrics import *\n",
    "import librosa\n",
    "import librosa.display\n",
    "import soundfile as sf\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import re\n",
    "\n",
    "print(\"Loading the sample vocal recording from the VCTK dataset...\")\n",
    "dataset = tfds.load(\"vctk\", with_info=False)\n",
    "sample_array = None\n",
    "transcript = None\n",
    "\n",
    "recording_index = 0\n",
    "chosen_recording = random.randint(0, 100)\n",
    "\n",
    "for sample in dataset['train']:\n",
    "    if recording_index == chosen_recording:\n",
    "        transcript = sample['text']\n",
    "        print(\"Recording transcript: {}\".format(transcript))\n",
    "        sample_array = np.array(sample['speech'], dtype=float)\n",
    "        break\n",
    "    recording_index += 1\n",
    "\n",
    "print(\"The selected sample's transcript: {}\".format(str(transcript)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Index of the recording: {}\".format(recording_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Sample array: {}\".format(sample_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sample_array_as_dataframe = pd.Series(sample_array)\n",
    "sample_array_as_dataframe.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As expected, most of the values are zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_array_as_dataframe_without_zeroes = pd.Series(sample_array[sample_array != 0])\n",
    "sample_array_as_dataframe_without_zeroes.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Even after filtering the zeroes, a lot of values still seem to be very close to 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "axes[0].boxplot(sample_array_as_dataframe)\n",
    "axes[1].boxplot(sample_array_as_dataframe_without_zeroes)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "The range of the values is enormous (-20000, 20000), so a boxplot doesn't really help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "axes[0].set_title(\"Histogram of the sample\")\n",
    "axes[0].hist(sample_array_as_dataframe, bins=100)\n",
    "\n",
    "axes[1].set_title(\"Histogram of the sample without zeroes\")\n",
    "axes[1].hist(sample_array_as_dataframe_without_zeroes, bins=100)\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We notice that there are lots of values centered around zero, which means that there is quite a lot of silence in a sample. In order to visualize the distribution in a clearer way, we can display the distribution of all values within and outside the interquartile range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "first_quartile = np.quantile(sample_array_as_dataframe_without_zeroes, 0.25, axis=0)\n",
    "third_quartile = np.quantile(sample_array_as_dataframe_without_zeroes, 0.75, axis=0)\n",
    "sample_array_as_dataframe_within_the_interquartile_range = pd.Series(sample_array_as_dataframe_without_zeroes[(sample_array_as_dataframe_without_zeroes > first_quartile) & (sample_array_as_dataframe_without_zeroes < third_quartile)])\n",
    "sample_array_as_dataframe_outside_the_interquartile_range = pd.Series(sample_array_as_dataframe_without_zeroes[(sample_array_as_dataframe_without_zeroes < first_quartile) | (sample_array_as_dataframe_without_zeroes > third_quartile)])\n",
    "sample_array_as_dataframe_within_the_interquartile_range.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sample_array_as_dataframe_outside_the_interquartile_range.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "axes[0].set_title(\"Histogram of the sample within the IQR\")\n",
    "axes[0].hist(sample_array_as_dataframe_within_the_interquartile_range, bins=100)\n",
    "axes[1].set_title(\"Histogram of the sample outside the IQR\")\n",
    "axes[1].hist(sample_array_as_dataframe_outside_the_interquartile_range, bins=100)\n",
    "\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "From the histogram displaying the distribution of values outside the IQR, we can deduce that most values are located somewhere between -10000 and +10000."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "axes[0].set_title(\"Boxplot for values within the IQR\")\n",
    "axes[0].boxplot(sample_array_as_dataframe_within_the_interquartile_range)\n",
    "axes[1].set_title(\"Boxplot for values outside the IQR\")\n",
    "axes[1].boxplot(sample_array_as_dataframe_outside_the_interquartile_range)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Checking the spectral content and what chunk size would fit for the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "\n",
    "audio_clip_spectrogram = librosa.feature.melspectrogram(sample_array, sr=VCTK_DATASET_SAMPLING_RATE)\n",
    "decibel_units = librosa.power_to_db(audio_clip_spectrogram, ref=np.max)\n",
    "figure, axes = plt.subplots(1, 1, figsize=(10,8))\n",
    "\n",
    "spectrogram_plot = librosa.display.specshow(decibel_units, x_axis='time',\n",
    "                    y_axis='mel', sr=VCTK_DATASET_SAMPLING_RATE, ax=axes)\n",
    "\n",
    "figure.colorbar(spectrogram_plot, ax=axes, format='%+2.0f dB')\n",
    "axes.set_title(\"{}\".format(str(transcript.numpy())[1:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can use IPython.display.Audio(...) to embed the recording in the Jupyter Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Audio\n",
    "\n",
    "full_recording_filename = \"exploratory-data-analysis-track-no-{}-high-res.wav\".format(chosen_recording)\n",
    "sf.write(\"outputs/exploratory-data-analysis/{}\".format(full_recording_filename), np.int16(sample_array), VCTK_DATASET_SAMPLING_RATE)\n",
    "\n",
    "Audio(\"outputs/exploratory-data-analysis/{}\".format(full_recording_filename), rate=VCTK_DATASET_SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Length of the sample recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(len(sample_array))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "content_after_one_second_filename = \"exploratory-data-analysis-track-no-{}-high-res-after-1-sec.wav\".format(chosen_recording)\n",
    "cropped_sample_array = sample_array[len(sample_array) - 2 * len(sample_array) // 3:]\n",
    "sf.write(\"outputs/exploratory-data-analysis/{}\".format(content_after_one_second_filename), np.int16(cropped_sample_array), VCTK_DATASET_SAMPLING_RATE)\n",
    "\n",
    "audio_clip_spectrogram = librosa.feature.melspectrogram(cropped_sample_array, sr=VCTK_DATASET_SAMPLING_RATE)\n",
    "decibel_units = librosa.power_to_db(audio_clip_spectrogram, ref=np.max)\n",
    "figure, axes = plt.subplots(1, 1, figsize=(10,8))\n",
    "\n",
    "spectrogram_plot = librosa.display.specshow(decibel_units, x_axis='time',\n",
    "                                            y_axis='mel', sr=VCTK_DATASET_SAMPLING_RATE, ax=axes)\n",
    "\n",
    "figure.colorbar(spectrogram_plot, ax=axes, format='%+2.0f dB')\n",
    "axes.set_title(\"{}\".format(str(transcript.numpy())[1:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Audio(\"outputs/exploratory-data-analysis/{}\".format(content_after_one_second_filename), rate=VCTK_DATASET_SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "approximately_one_second_of_the_sample = cropped_sample_array[: len(cropped_sample_array) // 2]\n",
    "\n",
    "first_second_clip_filename = \"exploratory-data-analysis-track-no-{}-high-res-first-sec.wav\".format(chosen_recording)\n",
    "sf.write(\"outputs/exploratory-data-analysis/{}\".format(first_second_clip_filename), np.int16(approximately_one_second_of_the_sample), VCTK_DATASET_SAMPLING_RATE)\n",
    "\n",
    "audio_clip_spectrogram = librosa.feature.melspectrogram(approximately_one_second_of_the_sample, sr=VCTK_DATASET_SAMPLING_RATE)\n",
    "decibel_units = librosa.power_to_db(audio_clip_spectrogram, ref=np.max)\n",
    "figure, axes = plt.subplots(1, 1, figsize=(10, 8))\n",
    "\n",
    "spectrogram_plot = librosa.display.specshow(decibel_units, x_axis='time',\n",
    "                                            y_axis='mel', sr=VCTK_DATASET_SAMPLING_RATE, ax=axes)\n",
    "\n",
    "figure.colorbar(spectrogram_plot, ax=axes, format='%+2.0f dB')\n",
    "axes.set_title(\"{}\".format(\"That's been our position a...\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Audio(\"outputs/exploratory-data-analysis/{}\".format(first_second_clip_filename), rate=VCTK_DATASET_SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Number of samples in a clip of approximately 1 second: {}\".format(len(approximately_one_second_of_the_sample)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We notice that the number of samples in the numpy array of approximately one second is close to 48000, so the sample rate of 48 kHz found on Google is accurate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For a chunk of 100 milliseconds (a tenth of a second) from a 48 kHz recording, we need $ \\frac{48000}{10} = 4800 $ samples.\n",
    "An interesting idea would be to try multiple chunk sizes, one for chunks of 100 milliseconds, one for chunks of 250 milliseconds, for chunks of 500 milliseconds and one for chunks of 1 second (for which the model would take more time and data to train).\n",
    "This is how a chunk of 100 milliseconds looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "chunk_of_100_ms = approximately_one_second_of_the_sample[:4800]\n",
    "\n",
    "clip_of_100_milliseconds_filename = \"exploratory-data-analysis-track-no-{}-high-res-first-100-ms.wav\".format(chosen_recording)\n",
    "sf.write(\"outputs/exploratory-data-analysis/{}\".format(clip_of_100_milliseconds_filename), np.int16(chunk_of_100_ms), VCTK_DATASET_SAMPLING_RATE)\n",
    "\n",
    "audio_clip_spectrogram = librosa.feature.melspectrogram(chunk_of_100_ms, sr=VCTK_DATASET_SAMPLING_RATE)\n",
    "decibel_units = librosa.power_to_db(audio_clip_spectrogram, ref=np.max)\n",
    "figure, axes = plt.subplots(1, 1, figsize=(10,8))\n",
    "\n",
    "spectrogram_plot = librosa.display.specshow(decibel_units, x_axis='time',\n",
    "                                            y_axis='mel', sr=VCTK_DATASET_SAMPLING_RATE, ax=axes)\n",
    "\n",
    "figure.colorbar(spectrogram_plot, ax=axes, format='%+2.0f dB')\n",
    "axes.set_title(\"{}\".format(str(transcript.numpy())[1:]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Obviously, the spectrogram doesn't really help in this case, because the audio clip is extremely short.\n",
    "Here is how it sounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "Audio(\"outputs/exploratory-data-analysis/{}\".format(clip_of_100_milliseconds_filename), rate=VCTK_DATASET_SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Indeed, it is a very short sound, but moving a sliding window all over a low-resolution recording to run the model on a small 100-milliseconds chunk in the execution script might work properly (remains to be seen later on after training)\n",
    "\n",
    "We can try downsampling the clip and perhaps also interpolating it afterwards to compare the spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "downsampled_array = np.array(sample_array[0::RESAMPLING_FACTOR])\n",
    "downsampled_recording_filename = \"exploratory-data-analysis-track-no-{}-downsampled-without-interpolation.wav\".format(chosen_recording)\n",
    "sf.write(\"outputs/exploratory-data-analysis/{}\".format(downsampled_recording_filename), np.int16(downsampled_array), VCTK_DATASET_SAMPLING_RATE)\n",
    "Audio(\"outputs/exploratory-data-analysis/{}\".format(downsampled_recording_filename), rate=VCTK_DATASET_SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Due to the recording getting downsampled by a factor of 4, the pitch of the voice in the WAV file sounds hilariously high and cartoonish. We have to adjust the sample rate properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sf.write(\"outputs/exploratory-data-analysis/{}\".format(downsampled_recording_filename), np.int16(downsampled_array), DOWNSAMPLED_RATE)\n",
    "Audio(\"outputs/exploratory-data-analysis/{}\".format(downsampled_recording_filename), rate=DOWNSAMPLED_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can compare the spectrograms of the high-res and low-res clips:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "high_res_audio_clip_spectrogram = librosa.feature.melspectrogram(sample_array, sr=VCTK_DATASET_SAMPLING_RATE)\n",
    "high_res_decibel_units = librosa.power_to_db(high_res_audio_clip_spectrogram, ref=np.max)\n",
    "low_res_audio_clip_spectrogram = librosa.feature.melspectrogram(downsampled_array, sr=DOWNSAMPLED_RATE)\n",
    "low_res_decibel_units = librosa.power_to_db(low_res_audio_clip_spectrogram, ref=np.max)\n",
    "figure, axes = plt.subplots(2, 1, figsize=(10, 8))\n",
    "\n",
    "axes[0].set_title(\"Low-res\")\n",
    "low_res_spectrogram_plot = librosa.display.specshow(low_res_decibel_units, x_axis='time',\n",
    "                                                     y_axis='mel', sr=DOWNSAMPLED_RATE, ax=axes[0])\n",
    "\n",
    "axes[1].set_title(\"High-res\")\n",
    "high_res_spectrogram_plot = librosa.display.specshow(high_res_decibel_units, x_axis='time',\n",
    "                            y_axis='mel', sr=VCTK_DATASET_SAMPLING_RATE, ax=axes[1])\n",
    "\n",
    "figure.tight_layout()\n",
    "figure.colorbar(low_res_spectrogram_plot, ax=[axes[0], axes[1]], format='%+2.0f dB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now we can also interpolate the downsampled signal and compare all 3 versions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "downsampled_and_interpolated_array = DatasetGenerator.upsample(downsampled_array, RESAMPLING_FACTOR)\n",
    "downsampled_recording_with_interpolation_filename = \"exploratory-data-analysis-track-no-{}-downsampled-with-interpolation.wav\".format(chosen_recording)\n",
    "sf.write(\"outputs/exploratory-data-analysis/{}\".format(downsampled_recording_with_interpolation_filename), np.int16(downsampled_and_interpolated_array), VCTK_DATASET_SAMPLING_RATE)\n",
    "Audio(\"outputs/exploratory-data-analysis/{}\".format(downsampled_recording_with_interpolation_filename), rate=VCTK_DATASET_SAMPLING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "high_res_audio_clip_spectrogram = librosa.feature.melspectrogram(sample_array, sr=VCTK_DATASET_SAMPLING_RATE)\n",
    "high_res_decibel_units = librosa.power_to_db(high_res_audio_clip_spectrogram, ref=np.max)\n",
    "low_res_audio_clip_spectrogram = librosa.feature.melspectrogram(downsampled_array, sr=DOWNSAMPLED_RATE)\n",
    "low_res_decibel_units = librosa.power_to_db(low_res_audio_clip_spectrogram, ref=np.max)\n",
    "low_res_with_interpolation_audio_clip_spectrogram = librosa.feature.melspectrogram(downsampled_and_interpolated_array, sr=DOWNSAMPLED_RATE)\n",
    "low_res_with_interpolation_decibel_units = librosa.power_to_db(low_res_with_interpolation_audio_clip_spectrogram, ref=np.max)\n",
    "figure, axes = plt.subplots(3, 1, figsize=(12, 8))\n",
    "\n",
    "axes[0].set_title(\"Low-res\")\n",
    "low_res_spectrogram_plot = librosa.display.specshow(low_res_decibel_units, x_axis='time',\n",
    "                                                    y_axis='mel', sr=DOWNSAMPLED_RATE, ax=axes[0])\n",
    "\n",
    "axes[1].set_title(\"Low-res and interpolated\")\n",
    "low_res_and_interpolated_spectrogram_plot = librosa.display.specshow(low_res_with_interpolation_decibel_units, x_axis='time',\n",
    "                                                    y_axis='mel', sr=VCTK_DATASET_SAMPLING_RATE, ax=axes[1])\n",
    "\n",
    "axes[2].set_title(\"High-res\")\n",
    "high_res_spectrogram_plot = librosa.display.specshow(high_res_decibel_units, x_axis='time',\n",
    "                                                     y_axis='mel', sr=VCTK_DATASET_SAMPLING_RATE, ax=axes[2])\n",
    "\n",
    "figure.tight_layout()\n",
    "figure.colorbar(low_res_spectrogram_plot, ax=[axes[0], axes[1], axes[2]], format='%+2.0f dB')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "One thing to notice is that interpolating the downsampled signal recovers a few of the high frequency components, but not enough of them.\n",
    "\n",
    "Downsampling the signal simply cuts off most of the frequency components above 4096 Hz.\n",
    "\n",
    "Obviously, the high-resolution recording has harmonics that are much more fine-grained and well-defined when compared to the low-res-and-interpolation version of the recording."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Some line plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(3, 2, figsize=(10, 8))\n",
    "\n",
    "axes[0, 0].set_title(\"High-res\")\n",
    "axes[0, 0].plot(sample_array)\n",
    "axes[1, 0].set_title(\"Low-res + interpolation\")\n",
    "axes[1, 0].plot(downsampled_and_interpolated_array)\n",
    "axes[2, 0].set_title(\"Low-res\")\n",
    "axes[2, 0].plot(downsampled_array)\n",
    "\n",
    "axes[0, 1].set_title(\"High-res FFT\")\n",
    "axes[0, 1].plot(np.fft.fft(sample_array))\n",
    "axes[1, 1].set_title(\"Low-res + interpolation FFT\")\n",
    "axes[1, 1].plot(np.fft.fft(downsampled_and_interpolated_array))\n",
    "axes[2, 1].set_title(\"Low-res FFT\")\n",
    "axes[2, 1].plot(np.fft.fft(downsampled_array))\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Of course, the number of samples in the downsampled array is 4 times lower.\n",
    "\n",
    "We can display the line plots for a small chunk of 100 milliseconds.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(3, 1, figsize=(10, 8))\n",
    "\n",
    "axes[0].set_title(\"Lineplot of the high-resolution 100 millisecond chunk\")\n",
    "axes[0].plot(chunk_of_100_ms)\n",
    "axes[1].set_title(\"Lineplot of the downsampled low-res 100 millisecond chunk\")\n",
    "axes[1].plot(chunk_of_100_ms[0::RESAMPLING_FACTOR])\n",
    "axes[2].set_title(\"Lineplot of the downsampled and interpolated 100 millisecond chunk\")\n",
    "axes[2].plot(DatasetGenerator.upsample(chunk_of_100_ms[0::RESAMPLING_FACTOR], RESAMPLING_FACTOR))\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "There's nothing that can be possibly inferred from this, so let's cut the chunk into smaller pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(3, 1, figsize=(10, 8))\n",
    "\n",
    "very_short_sample = chunk_of_100_ms[len(chunk_of_100_ms)-100:]\n",
    "axes[0].set_title(\"High-res line plot\")\n",
    "axes[0].plot(very_short_sample)\n",
    "axes[1].set_title(\"Low-res line plot\")\n",
    "axes[1].plot(very_short_sample[0::RESAMPLING_FACTOR])\n",
    "axes[2].set_title(\"Low-res + interpolation line plot\")\n",
    "axes[2].plot(DatasetGenerator.upsample(very_short_sample[0::RESAMPLING_FACTOR], RESAMPLING_FACTOR))\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "As expected, the low-resolution downsampled chunk has rough corners all over the place. The low-resolution + cubic spline interpolation version is smoother\n",
    "(probably due to the fact that cubic spline interpolation is used, instead of linear interpolation).\n",
    "\n",
    "It also makes sense why the old model trained on insanely short 256-sample-sized chunks is disappointing, because instead of actually feeding only\n",
    "downsampled chunks to the model directly, it applies cubic spline interpolation on the downsampled chunks, therefore smoothening the waveform instead. This\n",
    "is probably why the results of the old model seemed to have a very poor effect on high-frequency phonemes such as 'S'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "figure, axes = plt.subplots(3, 2, figsize=(24, 24))\n",
    "\n",
    "very_short_sample = chunk_of_100_ms[len(chunk_of_100_ms)-100:]\n",
    "\n",
    "axes[0, 0].set_title(\"High-res line plot\")\n",
    "axes[0, 0].plot(very_short_sample)\n",
    "axes[1, 0].set_title(\"Low-res line plot\")\n",
    "axes[1, 0].plot(very_short_sample[0::RESAMPLING_FACTOR])\n",
    "axes[2, 0].set_title(\"Low-res + interpolation line plot\")\n",
    "axes[2, 0].plot(DatasetGenerator.upsample(very_short_sample[0::RESAMPLING_FACTOR], RESAMPLING_FACTOR))\n",
    "axes[0, 1].set_title(\"High-res FFT\")\n",
    "axes[0, 1].plot(np.fft.fft(very_short_sample))\n",
    "axes[1, 1].set_title(\"Low-res FFT\")\n",
    "axes[1, 1].plot(np.fft.fft(very_short_sample[0::RESAMPLING_FACTOR]))\n",
    "axes[2, 1].set_title(\"Low-res + interpolation FFT\")\n",
    "axes[2, 1].plot(np.fft.fft(DatasetGenerator.upsample(very_short_sample[0::RESAMPLING_FACTOR], RESAMPLING_FACTOR)))\n",
    "\n",
    "figure.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}